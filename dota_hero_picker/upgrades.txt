    Add Factorization Machines for hero interactions: Implement a Factorization Machines layer (using the 2010 Rendle paper formula) in PyTorch to capture pairwise hero synergies and counters; integrate it after the team embeddings to model second-order interactions efficiently.
    ========================================================================================================================
    Training script probably can be optimized - investigate
    Optimize hyperparameters: Use Optuna to tune model hyperparameters (e.g., learning rate, embedding dimensions, FM latent factors) with a focus on personalization metrics, running trials on your validation set for 50-100 iterations.

    Add personalized hero attributes: For each hero in the draft, compute and concatenate user-specific features like historical win rate, play frequency, average damage dealt, and deaths from your past games, sourced from OpenDota or personal data; normalize these values for model stability.

    Incorporate hero roles into input vectors: Extract hero roles (e.g., Carry, Midlaner, Offlaner, Support) from OpenDota's heroes.json API and add them as one-hot encoded features or embeddings concatenated to each hero's slot in the input vector (team_vec, opponent_vec, pick_vec).



    Supplement with broader datasets: Download public pro/high-MMR match data from OpenDota or Kaggle datasets; merge with your personal match data, applying sample weighting (e.g., via PyTorch's WeightedRandomSampler) to prioritize your games during training.

    Train personalized hero embeddings: Use Word2Vec or CBOW on sequences of heroes from a mixed dataset (general pro matches + your games) to create embeddings that group heroes by synergy and your playstyle; concatenate these embeddings to the input vector instead of raw one-hot encodings.

    Apply chronological training splits and time-decay: Split your dataset by match timestamp or patch ID (using OpenDota's patch field) for train/validation/test sets; add time-decay weighting (e.g., exponential decay on older samples) in your loss function or sampler to emphasize recent metas.

    Calibrate model probabilities: After training, use scikit-learn's CalibratedClassifierCV with isotonic regression or Platt scaling on your validation set; evaluate with Brier score or log-loss to ensure reliable win-chance estimates.




    Implement Deep Sets for permutation-invariant team encoding: Read the Deep Sets paper (2017) and implement a simple Deep Sets layer in your model (e.g., via PyTorch nn.Module with phi and rho functions) to process each team's hero set independently, ensuring order doesn't leak.

    Build a two-tower architecture: Design your model with two separate Deep Sets towers (one for your team, one for opponent); combine their outputs with cross-features (e.g., concatenation or dot product) before feeding into a final classifier for win probability.

    Replace greedy selection with lookahead search: Implement a basic beam search or shallow Monte Carlo Tree Search (MCTS) in Python, using your model's calibrated probabilities to simulate opponent responses and evaluate multi-step draft outcomes.

    Update evaluation protocol: Compute AUC, Brier score, and per-patch performance on a held-out test set; add offline counterfactual tests by simulating draft perturbations and checking recommendation stability.

    Use differentiable F1 optimization: Implement a differentiable approximation of F1 (e.g., via sigmoid thresholding in a custom loss) to directly optimize it instead of BCE, which can help when F1 lags behind loss.



    Incorporate draft-time heuristics: Add checks in your inference code to enforce role completeness (e.g., penalize unbalanced teams missing a carry/support) and phase-aware priors (e.g., favor flex heroes early, counters late) based on the current pick stage.

    Ensemble models: Train 3-5 variants (e.g., with different seeds or subsets) and ensemble via averaging predictions; this often boosts F1 by 5-10% through diversity.

    Profile and optimize training: Use torch.profiler to check for bottlenecks (e.g., data loading, GPU usage); optimize batch sizes or use mixed precision (AMP) for faster iterations without sacrificing F1.

